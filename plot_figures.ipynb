{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_1samp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from nilearn import plotting\n",
    "from nilearn.image import resample_to_img\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.sparse import csgraph\n",
    "import cvxpy as cp\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deddd2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ses = 1\n",
    "sub = '04'\n",
    "run = 1\n",
    "\n",
    "base_path = '/mnt/TeamShare/Data_Masterfile/H20-00572_All-Dressed/PRECISIONSTIM_PD_Data_Results/fMRI_preprocessed_data/Rev_pipeline/derivatives'\n",
    "anat_img = nib.load(f'/mnt/TeamShare/Data_Masterfile/H20-00572_All-Dressed/PRECISIONSTIM_PD_Data_Results/fMRI_preprocessed_data/Rev_pipeline/derivatives/sub-pd0{sub}/ses-{ses}/anat/sub-pd0{sub}_ses-{ses}_T1w_brain.nii.gz')\n",
    "\n",
    "data_name = f'sub-pd0{sub}_ses-{ses}_run-{run}_task-mv_bold_corrected_smoothed_reg.nii.gz'\n",
    "BOLD_path_org = join(base_path, f'sub-pd0{sub}', f'ses-{ses}', 'func', data_name)\n",
    "bold_img = nib.load(BOLD_path_org)\n",
    "bold_data = bold_img.get_fdata()\n",
    "bold_data = bold_data.astype(np.float16)\n",
    "\n",
    "mask_path = f'/mnt/TeamShare/Data_Masterfile/H20-00572_All-Dressed/PRECISIONSTIM_PD_Data_Results/fMRI_preprocessed_data/Rev_pipeline/derivatives/sub-pd0{sub}/ses-{ses}/anat/sub-pd0{sub}_ses-{ses}_T1w_brain_mask.nii.gz'\n",
    "back_mask = nib.load(mask_path)\n",
    "back_mask = back_mask.get_fdata()\n",
    "back_mask = back_mask.astype(np.float16)\n",
    "\n",
    "mask_path = f'/mnt/TeamShare/Data_Masterfile/H20-00572_All-Dressed/PRECISIONSTIM_PD_Data_Results/fMRI_preprocessed_data/Rev_pipeline/derivatives/sub-pd0{sub}/ses-{ses}/anat/sub-pd0{sub}_ses-{ses}_T1w_brain_pve_0.nii.gz'\n",
    "csf_mask = nib.load(mask_path)\n",
    "csf_mask = csf_mask.get_fdata()\n",
    "csf_mask = csf_mask.astype(np.float16)\n",
    "\n",
    "mask_path = f'/mnt/TeamShare/Data_Masterfile/H20-00572_All-Dressed/PRECISIONSTIM_PD_Data_Results/fMRI_preprocessed_data/Rev_pipeline/derivatives/sub-pd0{sub}/ses-{ses}/anat/sub-pd0{sub}_ses-{ses}_T1w_brain_pve_1.nii.gz'\n",
    "white_mask = nib.load(mask_path)\n",
    "white_mask = white_mask.get_fdata()\n",
    "white_mask = white_mask.astype(np.float16)\n",
    "\n",
    "# print(anat_img.shape)\n",
    "# print(bold_data.shape)\n",
    "# print(back_mask.shape)\n",
    "# print(csf_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc899be",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_mask_data = back_mask > 0\n",
    "csf_mask_data = csf_mask > 0\n",
    "white_mask_data = white_mask > 0.5\n",
    "mask = np.logical_and(back_mask_data, ~csf_mask_data)\n",
    "mask &= ~white_mask_data\n",
    "nonzero_mask = np.where(mask)\n",
    "masked_bold = bold_data[nonzero_mask]\n",
    "\n",
    "print(f\"number of selected voxels after masking: {masked_bold.shape[0]/math.prod(bold_data.shape[:3])*100:.2f}%\")\n",
    "print('bold_data masked shape:', masked_bold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_img = nib.Nifti1Image(mask, bold_img.affine, bold_img.header)\n",
    "# mask_img = resample_to_img(mask_img, anat_img, interpolation=\"nearest\")\n",
    "\n",
    "# # interactive overlay\n",
    "# view = plotting.view_img(\n",
    "#     mask_img,\n",
    "#     bg_img=anat_img,\n",
    "#     cmap=\"autumn\",\n",
    "#     opacity=0.7,\n",
    "#     threshold=0,\n",
    "#     colorbar=False\n",
    "# )\n",
    "# view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_dict = np.load(f'TYPED_FITHRF_GLMDENOISE_RR.npy', allow_pickle=True).item()\n",
    "beta_glm = glm_dict['betasmd']\n",
    "beta_run1, beta_run2 = beta_glm[:,0,0,:90], beta_glm[:,0,0,90:]\n",
    "R2_run1, R2_run2 = glm_dict['R2run'][:,:,:,0], glm_dict['R2run'][:,:,:,1]\n",
    "\n",
    "mask = np.logical_and(back_mask_data, ~csf_mask_data)\n",
    "nonzero_mask = np.where(mask)\n",
    "white_mask_flat = white_mask_data[nonzero_mask]\n",
    "beta = beta_run1[~white_mask_flat]\n",
    "print(\"Beta Range:[\", np.nanmin(beta), np.nanmax(beta), \"], Mean: \", np.nanmean(beta))\n",
    "# plt.figure()\n",
    "# plt.hist(beta.flatten(), bins=100)\n",
    "# plt.title('Histogram of Beta Values (Run 1)')\n",
    "# plt.xlabel('Beta Value')\n",
    "R2 = R2_run1[~white_mask_flat]\n",
    "\n",
    "lower_thr, upper_thr = np.nanpercentile(beta, [1, 99])\n",
    "print(f'low_thr: {lower_thr:.2f}, high_thr: {upper_thr:.2f}') #low_thr: -4.64, high_thr: 4.60\n",
    "beta_extreme_mask = np.logical_or(beta < lower_thr, beta > upper_thr)\n",
    "voxels_with_extreme_beta = np.any(beta_extreme_mask, axis=1)\n",
    "\n",
    "print(f\"percentage of voxels with extreme beta values: {np.sum(voxels_with_extreme_beta)/beta.shape[0]*100:.2f}%\")\n",
    "\n",
    "mask = np.logical_and(back_mask_data, ~csf_mask_data)\n",
    "mask &= ~white_mask_data\n",
    "nonzero_mask = np.where(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = beta[voxels_with_extreme_beta]\n",
    "# print(\"Extreme Beta Range:[\", np.nanmin(tmp), np.nanmax(tmp), \"], Mean: \", np.nanmean(tmp))\n",
    "# plt.figure()\n",
    "# plt.hist(tmp.flatten())\n",
    "# plt.title('Histogram of Extreme Beta Values (Run 1)')\n",
    "# plt.xlabel('Beta Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e716a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme_volume = np.zeros(bold_img.shape[:3], dtype=np.float32)\n",
    "# extreme_volume[nonzero_mask] = voxels_with_extreme_beta.astype(np.float32)\n",
    "# extreme_img = nib.Nifti1Image(extreme_volume, bold_img.affine, bold_img.header)\n",
    "# extreme_img = resample_to_img(extreme_img, anat_img, interpolation='nearest')\n",
    "# view = plotting.view_img(\n",
    "#     extreme_img,\n",
    "#     bg_img=anat_img,\n",
    "#     cmap='autumn',\n",
    "#     symmetric_cmap=False,\n",
    "#     threshold=0.5,\n",
    "#     vmax=1,\n",
    "#     opacity=0.9,\n",
    "#     title='Voxels with Extreme Betas'\n",
    "# )\n",
    "# view\n",
    "# view.save_as_html(file_name=f'extreme_beta_voxels_sub{sub}_ses{ses}_run{run}.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after removing voxels with extreme beta values\n",
    "clean_beta = beta[~voxels_with_extreme_beta]\n",
    "clean_R2 = R2[~voxels_with_extreme_beta]\n",
    "print('clean_beta shape:', clean_beta.shape)\n",
    "\n",
    "# plt.hist(clean_beta.flatten(), bins=100)\n",
    "# plt.title('Histogram of Cleaned Beta Values (Run 1)')\n",
    "# plt.xlabel('Beta Value')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one sample t-test against 0\n",
    "tvals, pvals = ttest_1samp(clean_beta, popmean=0, axis=1, nan_policy='omit')\n",
    "\n",
    "# FDR correction\n",
    "tested = np.isfinite(pvals)\n",
    "alpha=0.05\n",
    "rej, q, _, _ = multipletests(pvals[tested], alpha=alpha, method='fdr_bh')\n",
    "\n",
    "n_voxel = clean_beta.shape[0]\n",
    "qvals  = np.full(n_voxel, np.nan)\n",
    "reject = np.zeros(n_voxel, dtype=bool)\n",
    "reject[tested] = rej\n",
    "qvals[tested]  = q\n",
    "\n",
    "# reject non-active voxels\n",
    "clean_active_beta = clean_beta[reject]\n",
    "clean_active_R2 = clean_R2[reject]\n",
    "print(f\"{clean_active_beta.shape[0]/clean_beta.shape[0]*100:.2f}% of voxels are active at FDR q<{alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e33fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map mean beta for active voxels back into anatomical space\n",
    "# clean_mask = ~np.asarray(voxels_with_extreme_beta, dtype=bool)\n",
    "# clean_indices = np.flatnonzero(clean_mask)\n",
    "# active_mask = np.asarray(reject, dtype=bool)\n",
    "# active_indices = clean_indices[active_mask]\n",
    "\n",
    "\n",
    "# active_voxel_coords = tuple(idx[active_indices] for idx in nonzero_mask)\n",
    "\n",
    "# mean_active_beta = np.nanmean(clean_active_beta, axis=1).astype(np.float32)\n",
    "# mean_beta_volume = np.full(bold_img.shape[:3], np.nan, dtype=np.float32)\n",
    "# mean_beta_volume[active_voxel_coords] = mean_active_beta\n",
    "\n",
    "# mean_beta_img = nib.Nifti1Image(mean_beta_volume, bold_img.affine, bold_img.header)\n",
    "# mean_beta_img = resample_to_img(mean_beta_img, anat_img, interpolation='linear')\n",
    "\n",
    "# finite_vals = np.isfinite(mean_active_beta)\n",
    "# if np.any(finite_vals):\n",
    "#     vmax = np.nanpercentile(mean_active_beta[finite_vals], 99)\n",
    "#     vmin = np.nanpercentile(mean_active_beta[finite_vals], 1)\n",
    "# else:\n",
    "#     vmax = vmin = 0.0\n",
    "\n",
    "# active_beta_view = plotting.view_img(\n",
    "#     mean_beta_img,\n",
    "#     bg_img=anat_img,\n",
    "#     cmap='seismic',\n",
    "#     symmetric_cmap=False,\n",
    "#     threshold=1e-6,\n",
    "#     vmax=vmax,\n",
    "#     vmin=vmin,\n",
    "#     colorbar=True,\n",
    "#     title='Mean beta for active voxels'\n",
    "# )\n",
    "# active_beta_view\n",
    "# # active_beta_view.save_as_html(file_name=f'active_beta_map_sub{sub}_ses{ses}_run{run}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualise which voxels survived the active-voxel selection\n",
    "# active_mask_volume = np.zeros(bold_img.shape[:3], dtype=np.uint8)\n",
    "# active_mask_volume[active_voxel_coords] = 1\n",
    "\n",
    "# active_mask_img = nib.Nifti1Image(active_mask_volume.astype(np.float32), bold_img.affine, bold_img.header)\n",
    "# active_mask_img = resample_to_img(active_mask_img, anat_img, interpolation='nearest')\n",
    "\n",
    "# active_mask_view = plotting.view_img(\n",
    "#     active_mask_img,\n",
    "#     bg_img=anat_img,\n",
    "#     cmap='autumn',\n",
    "#     symmetric_cmap=False,\n",
    "#     threshold=0.5,\n",
    "#     vmax=1,\n",
    "#     opacity=0.9,\n",
    "#     title='Active voxels mask'\n",
    "# )\n",
    "# active_mask_view\n",
    "# active_mask_view.save_as_html(file_name=f'active_voxels_sub{sub}_ses{ses}_run{run}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e58747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer back beta value on the volume\n",
    "clean_mask = ~np.asarray(voxels_with_extreme_beta, dtype=bool)\n",
    "clean_indices = np.nonzero(clean_mask)[0]\n",
    "active_indices = clean_indices[np.asarray(reject, dtype=bool)]\n",
    "\n",
    "spatial_shape = bold_img.shape[:3]\n",
    "n_trials = clean_active_beta.shape[1]\n",
    "beta_volume = np.full(spatial_shape + (n_trials,), np.nan, dtype=np.float32)\n",
    "\n",
    "coords = tuple(axis[active_indices] for axis in nonzero_mask)\n",
    "beta_volume[coords[0], coords[1], coords[2], :] = clean_active_beta.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46270fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hampel_filter_image(image, window_size, threshold_factor):\n",
    "#     if window_size % 2 == 0:\n",
    "#         raise ValueError(\"window_size must be odd\")\n",
    "        \n",
    "#     filtered = image.astype(float).copy()\n",
    "#     footprint = np.ones((window_size,)*3, dtype=bool)\n",
    "\n",
    "#     print(image.shape[3])\n",
    "#     for t in range(image.shape[3]):\n",
    "#         print(t)\n",
    "#         vol = image[..., t]\n",
    "#         med = ndimage.generic_filter(vol, np.nanmedian, footprint=footprint, mode='constant', cval=np.nan)\n",
    "#         mad = ndimage.generic_filter(np.abs(vol - med), np.nanmedian, footprint=footprint, mode='constant', cval=np.nan)\n",
    "\n",
    "#         scaled_mad = 1.4826 * mad  # Gaussian-consistent scaling\n",
    "#         valid = ~np.isnan(vol)\n",
    "#         outliers = valid & (np.abs(vol - med) > threshold_factor * scaled_mad)\n",
    "#         filtered[..., t][outliers] = np.nan\n",
    "#     return filtered\n",
    "\n",
    "def hampel_filter_image(image, window_size, threshold_factor):\n",
    "    if window_size % 2 == 0:\n",
    "        raise ValueError(\"window_size must be odd\")\n",
    "\n",
    "    filtered = image.astype(float).copy()\n",
    "    footprint = np.ones((window_size,) * 3, dtype=bool)\n",
    "\n",
    "    for t in range(image.shape[3]):\n",
    "        print(t)\n",
    "        vol = image[..., t]\n",
    "        med = ndimage.generic_filter(vol, np.nanmedian, footprint=footprint, mode='constant', cval=np.nan)\n",
    "        mad = ndimage.generic_filter(np.abs(vol - med), np.nanmedian, footprint=footprint, mode='constant', cval=np.nan)\n",
    "        counts = ndimage.generic_filter(np.isfinite(vol).astype(np.float32), np.sum, footprint=footprint, mode='constant', cval=0)\n",
    "\n",
    "        scaled_mad = 1.4826 * mad  # Gaussian-consistent scaling\n",
    "        valid = np.isfinite(vol)\n",
    "        insufficient = counts < 3\n",
    "\n",
    "        filtered[..., t][insufficient] = np.nan\n",
    "\n",
    "        enough_data = (counts >= 3) & valid\n",
    "        outliers = enough_data & (np.abs(vol - med) > threshold_factor * scaled_mad)\n",
    "        filtered[..., t][outliers] = med[outliers]\n",
    "    return filtered\n",
    "beta_volume_filter = hampel_filter_image(beta_volume, window_size=5, threshold_factor=3)\n",
    "np.save(f'cleaned_beta_volume_sub{sub}_ses{ses}_run{run}.npy', beta_volume_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_volume_filter = np.load(\"beta_volume_filter.npy\")\n",
    "bold_data = bold_img.get_fdata()\n",
    "beta_volume_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_volume_filter = beta_volume_filter.astype(np.float16)\n",
    "spatial_shape = beta_volume_filter.shape[:-1]\n",
    "voxels_with_any_nan = np.zeros(spatial_shape, dtype=bool)\n",
    "voxels_with_all_nan = np.ones(spatial_shape, dtype=bool)\n",
    "\n",
    "# Sweep the time dimension once\n",
    "for t in range(beta_volume_filter.shape[-1]):\n",
    "    frame_nan = np.isnan(beta_volume_filter[..., t])\n",
    "    voxels_with_any_nan |= frame_nan\n",
    "    voxels_with_all_nan &= frame_nan\n",
    "\n",
    "print(np.sum(voxels_with_any_nan), np.sum(voxels_with_all_nan), flush=True)\n",
    "\n",
    "n_trial = beta_volume_filter.shape[-1]\n",
    "beta_volume_filter_2d = beta_volume_filter.reshape(-1, n_trial)\n",
    "print(beta_volume_filter_2d.shape, flush=True)\n",
    "mask_2d = voxels_with_all_nan.reshape(-1)\n",
    "beta_valume_clean_2d = beta_volume_filter_2d[~mask_2d]\n",
    "print(beta_valume_clean_2d.shape, flush=True)\n",
    "np.save(f'beta_valume_clean_2d.npy', beta_valume_clean_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965685ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "del anat_img, back_mask, csf_mask, white_mask, mask, nonzero_mask, masked_bold\n",
    "del R2_run1, R2_run2, beta_run1, beta_run2, R2, beta, clean_beta, clean_R2, voxels_with_all_nan, voxels_with_any_nan\n",
    "del beta_volume_filter_2d, beta_valume_clean_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare clean_active_beta with the filtered volume to quantify NaNs introduced by filtering\n",
    "# filtered_active_beta = beta_volume_filter[coords[0], coords[1], coords[2], :]\n",
    "# filtered_active_beta = filtered_active_beta.reshape(clean_active_beta.shape)\n",
    "\n",
    "# nan_before = np.isnan(clean_active_beta)\n",
    "# nan_after = np.isnan(filtered_active_beta)\n",
    "\n",
    "# voxels_before_nan = np.sum(nan_before.any(axis=1))\n",
    "# voxels_after_nan = np.sum(nan_after.any(axis=1))\n",
    "# new_nan_voxels = np.sum(~nan_before.any(axis=1) & nan_after.any(axis=1))\n",
    "\n",
    "# voxel_count = clean_active_beta.shape[0]\n",
    "# entry_count = clean_active_beta.size\n",
    "\n",
    "# # print(f\"Active voxels evaluated: {voxel_count}\")\n",
    "# # print(f\"Voxels with NaNs before filtering: {voxels_before_nan}\")\n",
    "# # print(f\"Voxels with NaNs after filtering: {voxels_after_nan}\")\n",
    "# # print(f\"New voxels turned NaN by filtering: {new_nan_voxels}\")\n",
    "# # print(f\"Fraction of voxels with any NaN after filtering: {voxels_after_nan/voxel_count:.2%}\")\n",
    "# # print(f\"NaN entries before filtering: {nan_before.sum()} / {entry_count}\")\n",
    "# print(f\"{nan_after.sum() / entry_count *100: .2f}% of voxels are filtered\")\n",
    "# # print(f\"New NaN entries introduced by filtering: {(~nan_before & nan_after).sum()} / {entry_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_valume_clean_2d = np.load(f'beta_valume_clean_2d.npy')\n",
    "beta_valume_clean_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5245630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bold_data_reshape = bold_data.reshape(-1, bold_data.shape[-1])\n",
    "# bold_data_reshape = bold_data_reshape.astype(np.float16)\n",
    "# bold_data_selected = bold_data_reshape[~mask_2d]         # keep voxels of interest\n",
    "# bold_data_selected = bold_data_selected.astype(np.float16)\n",
    "# trial_len = 9\n",
    "# num_trials = bold_data.shape[-1]\n",
    "# trial_idx = np.arange(num_trials)\n",
    "# bold_data_selected_reshape = np.zeros((bold_data_selected.shape[0], num_trials, trial_len), dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f09a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = 0\n",
    "\n",
    "# for i in range(num_trials):\n",
    "#     end = start + trial_len\n",
    "#     if end > bold_data_selected.shape[1]:\n",
    "#         raise ValueError(\"BOLD data does not contain enough timepoints for all trials\")\n",
    "#     bold_data_selected_reshape[:, i, :] = bold_data_selected[:, start:end]\n",
    "#     start += trial_len\n",
    "#     if start == 270 or start == 560:   # your skips\n",
    "#         start += 20\n",
    "# X = bold_data_selected_reshape[:, trial_idx, :]          # [Nvox, Ntrials, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde94970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# X = np.load('X.npy')\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f34709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA()\n",
    "# X_reshap = X.reshape(X.shape[0], -1).astype(np.float32)\n",
    "# X_pca = pca.fit_transform(X_reshap.T) #(810, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303811bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA Explained Variance\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "tmp = np.cumsum(pca.explained_variance_ratio_)\n",
    "np.where(tmp>0.95)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b26f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "weights = np.load('voxel_space_weights.npy')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9bd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Find extreme percentile voxels and plot them over anatomy.\"\"\"\n",
    "import sys\n",
    "import site\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_PATH = Path(__file__).resolve().parent\n",
    "ANAT_PATH = BASE_PATH / \"sub-pd004_ses-1_T1w_brain.nii.gz\"\n",
    "BETA_PATH = BASE_PATH / \"beta_volume_filter.npy\"\n",
    "WEIGHT_PATH = BASE_PATH / \"voxel_space_weights.npy\"\n",
    "FIG_PATH = BASE_PATH / \"extreme_voxels_overlay.png\"\n",
    "\n",
    "LOW_LABEL = \"1st percentile\"\n",
    "HIGH_LABEL = \"99th percentile\"\n",
    "\n",
    "\n",
    "def load_valid_mask(beta_path: Path) -> np.ndarray:\n",
    "    beta_volume = np.load(beta_path, mmap_mode=\"r\")\n",
    "    valid = ~np.all(np.isnan(beta_volume), axis=-1)\n",
    "    return valid\n",
    "\n",
    "\n",
    "def percentile_masks(weights: np.ndarray, percentiles=(1, 99)):\n",
    "    lower_thr, upper_thr = np.percentile(weights, percentiles)\n",
    "    lower_mask = weights <= lower_thr\n",
    "    upper_mask = weights >= upper_thr\n",
    "    return lower_thr, upper_thr, lower_mask, upper_mask\n",
    "\n",
    "\n",
    "def reshape_mask(mask_flat: np.ndarray, shape):\n",
    "    return mask_flat.reshape(shape)\n",
    "\n",
    "\n",
    "def pick_center(coords: np.ndarray, shape):\n",
    "    if coords.size == 0:\n",
    "        return np.array([dim // 2 for dim in shape], dtype=int)\n",
    "    return np.median(coords, axis=0).astype(int)\n",
    "\n",
    "\n",
    "def extract_slice(volume: np.ndarray, index: int, axis: int) -> np.ndarray:\n",
    "    if axis == 0:\n",
    "        slc = volume[index, :, :]\n",
    "    elif axis == 1:\n",
    "        slc = volume[:, index, :]\n",
    "    else:\n",
    "        slc = volume[:, :, index]\n",
    "    return np.rot90(slc)\n",
    "\n",
    "\n",
    "def overlay_plot(anat_data: np.ndarray, low_mask: np.ndarray, high_mask: np.ndarray, center: np.ndarray, fig_path: Path):\n",
    "    orientations = [(0, \"Sagittal\"), (1, \"Coronal\"), (2, \"Axial\")]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    for ax, (axis, title) in zip(axes, orientations):\n",
    "        idx = int(center[axis])\n",
    "        anat_slice = extract_slice(anat_data, idx, axis)\n",
    "        low_slice = extract_slice(low_mask.astype(float), idx, axis)\n",
    "        high_slice = extract_slice(high_mask.astype(float), idx, axis)\n",
    "\n",
    "        ax.imshow(anat_slice, cmap=\"gray\", origin=\"lower\")\n",
    "        if np.any(low_slice):\n",
    "            ax.imshow(np.ma.masked_where(low_slice == 0, low_slice), cmap=\"Blues\", alpha=0.6, origin=\"lower\")\n",
    "        if np.any(high_slice):\n",
    "            ax.imshow(np.ma.masked_where(high_slice == 0, high_slice), cmap=\"Reds\", alpha=0.6, origin=\"lower\")\n",
    "        ax.set_title(f\"{title} slice (index {idx})\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    handles = [plt.Line2D([0], [0], color=color, lw=4) for color in (\"blue\", \"red\")]\n",
    "    labels = [LOW_LABEL, HIGH_LABEL]\n",
    "    fig.legend(handles, labels, loc=\"upper center\", ncol=2)\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.92))\n",
    "    fig.savefig(fig_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not WEIGHT_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Missing weight file: {WEIGHT_PATH}\")\n",
    "\n",
    "    weights = np.load(WEIGHT_PATH)\n",
    "    lower_thr, upper_thr, low_mask_flat, high_mask_flat = percentile_masks(weights)\n",
    "\n",
    "    valid_mask = load_valid_mask(BETA_PATH)\n",
    "    valid_flat = valid_mask.reshape(-1)\n",
    "\n",
    "    low_full_flat = np.zeros_like(valid_flat, dtype=bool)\n",
    "    high_full_flat = np.zeros_like(valid_flat, dtype=bool)\n",
    "    low_full_flat[valid_flat] = low_mask_flat\n",
    "    high_full_flat[valid_flat] = high_mask_flat\n",
    "\n",
    "    volume_shape = valid_mask.shape\n",
    "    low_volume = reshape_mask(low_full_flat, volume_shape)\n",
    "    high_volume = reshape_mask(high_full_flat, volume_shape)\n",
    "\n",
    "    anat_img = nib.load(ANAT_PATH)\n",
    "    anat_data = anat_img.get_fdata()\n",
    "\n",
    "    low_coords = np.argwhere(low_volume)\n",
    "    high_coords = np.argwhere(high_volume)\n",
    "    stacked = np.vstack([low_coords, high_coords]) if low_coords.size and high_coords.size else \\\n",
    "              (low_coords if low_coords.size else high_coords)\n",
    "    center = pick_center(stacked, volume_shape)\n",
    "\n",
    "    overlay_plot(anat_data, low_volume, high_volume, center, FIG_PATH)\n",
    "\n",
    "    print(f\"Lower threshold ({LOW_LABEL}): {lower_thr:.6f}\")\n",
    "    print(f\"Upper threshold ({HIGH_LABEL}): {upper_thr:.6f}\")\n",
    "    print(f\"Low percentile voxels: {low_coords.shape[0]}\")\n",
    "    print(f\"High percentile voxels: {high_coords.shape[0]}\")\n",
    "    print(f\"Overlay figure saved to {FIG_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del beta_valume_clean_2d\n",
    "# beta_valume_clean_2d = np.load(f'beta_valume_clean_2d.npy')\n",
    "# print(beta_valume_clean_2d.shape)\n",
    "# print(f\"beta reduced range: \", beta_valume_clean_2d.min(), beta_valume_clean_2d.max())\n",
    "\n",
    "# components = pca.components_[:600]\n",
    "# mean = pca.mean_\n",
    "# print(mean.shape)\n",
    "# print(components.shape)\n",
    "\n",
    "\n",
    "# # beta_reduced = pca.transform(beta_valume_clean_2d.T)\n",
    "# beta_reduced = (beta_valume_clean_2d.T - mean) @ components.T\n",
    "# beta_reduced = beta_reduced.T\n",
    "# print(beta_reduced.shape)\n",
    "# print(f\"beta reduced range: \", beta_reduced.min(), beta_reduced.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the mean filtered beta volume on the anatomical brain\n",
    "# if beta_volume_filter.ndim == 4 and beta_volume_filter.shape[-1] > 1:\n",
    "#     with np.errstate(invalid='ignore'):\n",
    "#         beta_filter_mean = np.nanmean(beta_volume_filter, axis=-1)\n",
    "# else:\n",
    "#     beta_filter_mean = np.asanyarray(beta_volume_filter).squeeze()\n",
    "\n",
    "# beta_filter_mean = beta_filter_mean.astype(np.float32)\n",
    "\n",
    "# beta_filter_img = nib.Nifti1Image(beta_filter_mean, bold_img.affine, bold_img.header)\n",
    "# beta_filter_img = resample_to_img(beta_filter_img, anat_img, interpolation='linear')\n",
    "\n",
    "# finite_vals = np.isfinite(beta_filter_mean)\n",
    "# if np.any(finite_vals):\n",
    "#     vmax = np.nanpercentile(beta_filter_mean[finite_vals], 99)\n",
    "#     vmin = np.nanpercentile(beta_filter_mean[finite_vals], 1)\n",
    "# else:\n",
    "#     vmax = vmin = 0.0\n",
    "\n",
    "# beta_filter_view = plotting.view_img(\n",
    "#     beta_filter_img,\n",
    "#     bg_img=anat_img,\n",
    "#     cmap='seismic',\n",
    "#     symmetric_cmap=False,\n",
    "#     threshold=1e-6,\n",
    "#     vmax=vmax,\n",
    "#     vmin=vmin,\n",
    "#     colorbar=True,\n",
    "#     title='Mean filtered beta volume'\n",
    "# )\n",
    "# beta_filter_view\n",
    "# beta_filter_view.save_as_html(file_name=f'filtered_beta_map_sub{sub}_ses{ses}_run{run}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce177ee4",
   "metadata": {},
   "source": [
    "The right one before PCA (L_smooth doesn't look correct. You don't use the anat file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84298667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_matrices(beta_valume_clean_2d, bold_data, anat_img, mask_2d, trial_indices=None, trial_len=9):\n",
    "#     num_trials = beta_valume_clean_2d.shape[-1]\n",
    "#     trial_idx = np.arange(num_trials) if trial_indices is None else np.unique(np.asarray(trial_indices, int).ravel())\n",
    "\n",
    "#     # ----- L_task (same idea as yours) -----\n",
    "#     beta_selected = beta_valume_clean_2d[:, trial_idx]\n",
    "#     counts = np.count_nonzero(np.isfinite(beta_selected), axis=-1)\n",
    "#     sums = np.nansum(beta_selected, axis=-1, dtype=np.float64)\n",
    "#     mean_beta = np.zeros(beta_selected.shape[0], dtype=np.float32)\n",
    "#     m = counts > 0\n",
    "#     mean_beta[m] = (sums[m] / counts[m]).astype(np.float32)\n",
    "#     L_task = np.zeros_like(mean_beta, dtype=np.float32)\n",
    "#     v = np.abs(mean_beta) > 0\n",
    "#     L_task[v] = (1.0 / np.abs(mean_beta[v])).astype(np.float32)\n",
    "\n",
    "#     # ----- reshape BOLD into trials -----\n",
    "#     bold_data_reshape = bold_data.reshape(-1, bold_data.shape[-1])\n",
    "#     bold_data_selected = bold_data_reshape[~mask_2d]         # keep voxels of interest\n",
    "#     bold_data_selected_reshape = np.zeros((bold_data_selected.shape[0], num_trials, trial_len), dtype=np.float32)\n",
    "#     start = 0\n",
    "#     for i in range(num_trials):\n",
    "#         end = start + trial_len\n",
    "#         if end > bold_data_selected.shape[1]:\n",
    "#             raise ValueError(\"BOLD data does not contain enough timepoints for all trials\")\n",
    "#         bold_data_selected_reshape[:, i, :] = bold_data_selected[:, start:end]\n",
    "#         start += trial_len\n",
    "#         if start == 270 or start == 560:   # your skips\n",
    "#             start += 20\n",
    "#     X = bold_data_selected_reshape[:, trial_idx, :]          # [Nvox, Ntrials, T]\n",
    "\n",
    "#     # ----- L_var: variance of trial differences, as sparse diagonal -----\n",
    "#     m = np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)\n",
    "#     for i in range(num_trials-1):\n",
    "#         x1 = X[:, i, :]\n",
    "#         x2 = X[:, i+1, :]\n",
    "#         m += (x1-x2) @ (x1-x2).T\n",
    "#     L_var /= (num_trials - 1)\n",
    "\n",
    "#     # diff_mat = np.diff(X, axis=1)\n",
    "#     # diff_mat_flat = diff_mat.reshape(diff_mat.shape[0], -1)\n",
    "#     # L_var = np.cov(diff_mat_flat, bias=False, dtype=np.float32)\n",
    "#     # L_var = (L_var + L_var.T) / 2 + 1e-6 * np.eye(L_var.shape[0], dtype=np.float32)\n",
    "\n",
    "#     # ----- L_smooth: sparse 6-neighbor Laplacian on the voxel grid -----\n",
    "#     mask3d = (~mask_2d).reshape(bold_data.shape[:3])\n",
    "#     idx = -np.ones(mask3d.shape, dtype=np.int64)\n",
    "#     idx[mask3d] = np.arange(mask3d.sum())\n",
    "#     rows, cols = [], []\n",
    "#     for ax in range(3):\n",
    "#         s1 = [slice(None)]*3; s2 = [slice(None)]*3\n",
    "#         s1[ax] = slice(1, None); s2[ax] = slice(0, -1)\n",
    "#         a, b = idx[tuple(s1)], idx[tuple(s2)]\n",
    "#         m = (a != -1) & (b != -1)\n",
    "#         i, j = a[m].ravel(), b[m].ravel()\n",
    "#         rows.append(np.concatenate([i, j])); cols.append(np.concatenate([j, i]))\n",
    "#     if rows:\n",
    "#         rows, cols = np.concatenate(rows), np.concatenate(cols)\n",
    "#         A = sp.coo_matrix((np.ones(rows.size, np.float32), (rows, cols)), shape=(idx.max()+1, idx.max()+1)).tocsr()\n",
    "#         d = np.asarray(A.sum(axis=1)).ravel()\n",
    "#         L_smooth = (sp.diags(d, 0) - A).astype(np.float32).tocsc()\n",
    "#     else:\n",
    "#         L_smooth = sp.csc_matrix((idx.max()+1, idx.max()+1), dtype=np.float32)\n",
    "\n",
    "#     selected_BOLD_flat = X.reshape(X.shape[0], -1).astype(np.float32)\n",
    "#     return L_task.astype(np.float32), L_var, L_smooth, selected_BOLD_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03969e0",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89957174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- apply PCA ----\n",
    "# X = np.load('X.npy')\n",
    "# pca = PCA()\n",
    "# X_reshap = X.reshape(X.shape[0], -1).astype(np.float32)\n",
    "# X_pca = pca.fit_transform(X_reshap.T) #(810, 800)\n",
    "\n",
    "# beta_valume_clean_2d = np.load(f'beta_valume_clean_2d.npy')\n",
    "# components = pca.components_[:600]\n",
    "# mean = pca.mean_\n",
    "# beta_reduced = (beta_valume_clean_2d.T - mean) @ components.T\n",
    "# beta_reduced = beta_reduced.T\n",
    "\n",
    "# num_trials = beta_valume_clean_2d.shape[-1]\n",
    "# trial_idx = np.arange(num_trials)\n",
    "# trial_len = 9\n",
    "\n",
    "# # ----- L_task (same idea as yours) -----\n",
    "# beta_selected = beta_reduced[:, trial_idx]\n",
    "# counts = np.count_nonzero(np.isfinite(beta_selected), axis=-1)\n",
    "# sums = np.nansum(beta_selected, axis=-1, dtype=np.float64)\n",
    "# mean_beta = np.zeros(beta_selected.shape[0], dtype=np.float32)\n",
    "# m = counts > 0\n",
    "# mean_beta[m] = (sums[m] / counts[m]).astype(np.float32)\n",
    "# L_task = np.zeros_like(mean_beta, dtype=np.float32)\n",
    "# v = np.abs(mean_beta) > 0\n",
    "# L_task[v] = (1.0 / np.abs(mean_beta[v])).astype(np.float32)\n",
    "\n",
    "# # ----- L_var: variance of trial differences, as sparse diagonal -----\n",
    "# X_pca = X_pca[:, :600].T\n",
    "# X = X_pca.reshape(X_pca.shape[0], num_trials, trial_len)\n",
    "# L_var = np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)\n",
    "# for i in range(num_trials-1):\n",
    "#     x1 = X[:, i, :]\n",
    "#     x2 = X[:, i+1, :]\n",
    "#     L_var += (x1-x2) @ (x1-x2).T\n",
    "# L_var /= (num_trials - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce552248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrices(beta_valume_clean_2d, bold_data, mask_2d, trial_indices=None, trial_len=9, num_components=600):\n",
    "    num_trials = beta_valume_clean_2d.shape[-1]\n",
    "    trial_idx = np.arange(num_trials) if trial_indices is None else np.unique(np.asarray(trial_indices, int).ravel())\n",
    "    pca = PCA()\n",
    "    X_reshap = X.reshape(X.shape[0], -1).astype(np.float32)\n",
    "    X_pca = pca.fit_transform(X_reshap.T) #(810, 800)\n",
    "\n",
    "    components = pca.components_[:num_components]\n",
    "    mean = pca.mean_\n",
    "    beta_reduced = (beta_valume_clean_2d.T - mean) @ components.T\n",
    "    beta_reduced = beta_reduced.T\n",
    "\n",
    "\n",
    "    # ----- L_task (same idea as yours) -----\n",
    "    beta_selected = beta_reduced[:, trial_idx]\n",
    "    counts = np.count_nonzero(np.isfinite(beta_selected), axis=-1)\n",
    "    sums = np.nansum(beta_selected, axis=-1, dtype=np.float64)\n",
    "    mean_beta = np.zeros(beta_selected.shape[0], dtype=np.float32)\n",
    "    m = counts > 0\n",
    "    mean_beta[m] = (sums[m] / counts[m]).astype(np.float32)\n",
    "    L_task = np.zeros_like(mean_beta, dtype=np.float32)\n",
    "    v = np.abs(mean_beta) > 0\n",
    "    L_task[v] = (1.0 / np.abs(mean_beta[v])).astype(np.float32)\n",
    "\n",
    "    # ----- reshape BOLD into trials -----\n",
    "    bold_data_reshape = bold_data.reshape(-1, bold_data.shape[-1])\n",
    "    bold_data_selected = bold_data_reshape[~mask_2d]         # keep voxels of interest\n",
    "    bold_data_selected_reshape = np.zeros((bold_data_selected.shape[0], num_trials, trial_len), dtype=np.float32)\n",
    "    start = 0\n",
    "    for i in range(num_trials):\n",
    "        end = start + trial_len\n",
    "        if end > bold_data_selected.shape[1]:\n",
    "            raise ValueError(\"BOLD data does not contain enough timepoints for all trials\")\n",
    "        bold_data_selected_reshape[:, i, :] = bold_data_selected[:, start:end]\n",
    "        start += trial_len\n",
    "        if start == 270 or start == 560:   # your skips\n",
    "            start += 20\n",
    "    X = bold_data_selected_reshape[:, trial_idx, :]          # [Nvox, Ntrials, T]\n",
    "\n",
    "    # ----- L_var: variance of trial differences, as sparse diagonal -----\n",
    "    X_pca = X_pca[:, :600].T\n",
    "    X = X_pca.reshape(X_pca.shape[0], num_trials, trial_len)\n",
    "    L_var = np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)\n",
    "    for i in range(num_trials-1):\n",
    "        x1 = X[:, i, :]\n",
    "        x2 = X[:, i+1, :]\n",
    "        L_var += (x1-x2) @ (x1-x2).T\n",
    "    L_var /= (num_trials - 1)\n",
    "\n",
    "    selected_BOLD_flat = X.reshape(X.shape[0], -1).astype(np.float32)\n",
    "    return L_task.astype(np.float32), L_var, selected_BOLD_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(w, L_task, L_var, L_smooth, alpha_var, alpha_smooth):\n",
    "    quad = (w.T @ np.diag(L_task) @ w + alpha_var * (w.T @ L_var @ w) + alpha_smooth * (w.T @ L_smooth @ w))\n",
    "    return quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f47abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_voxel_weights(L_task, L_var, L_smooth, alpha_var, alpha_smooth):\n",
    "    L_total = np.diag(L_task) + alpha_var * L_var + alpha_smooth * L_smooth\n",
    "    n = L_total.shape[0]\n",
    "    L_total = np.nan_to_num(L_total)\n",
    "    L_total = 0.5*(L_total + L_total.T) + 1e-8*np.eye(n)\n",
    "    w = cp.Variable(n, nonneg=True)\n",
    "    constraints = [cp.sum(w) == 1]\n",
    "\n",
    "    # objective = cp.Minimize(cp.quad_form(w, L_total) + alpha_sparse * cp.norm1(w))\n",
    "    objective = cp.Minimize(cp.quad_form(w, L_total))\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver=cp.OSQP, verbose=True)\n",
    "    return w.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ee7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight(param_grid, beta_valume_clean_2d, bold_data, anat_img, mask_2d, trial_len):\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=0)\n",
    "    best_score = np.inf\n",
    "    best_params = None\n",
    "    num_trials = beta_valume_clean_2d.shape[-1]\n",
    "\n",
    "    for a_var, a_smooth in product(*param_grid.values()):\n",
    "        fold_scores = []\n",
    "        print(f\"a_var: {a_var}, a_smooth: {a_smooth}\")\n",
    "        count = 1\n",
    "\n",
    "        for train_idx, val_idx in kf.split(np.arange(num_trials)):\n",
    "            print(f\"k-fold num: {count}\")\n",
    "            L_task_train, L_var_train, L_smooth_train, _ = calculate_matrices(beta_valume_clean_2d, bold_data, anat_img, mask_2d, train_idx, trial_len)\n",
    "            w = optimize_voxel_weights(L_task_train, L_var_train, L_smooth_train, alpha_var=a_var, alpha_smooth=a_smooth)\n",
    "\n",
    "            L_task_val, L_var_val, L_smooth_val, _ = calculate_matrices(beta_valume_clean_2d, bold_data, anat_img, mask_2d, val_idx, trial_len)\n",
    "\n",
    "            fold_scores.append(objective_func(w, L_task_val, L_var_val, L_smooth_val, a_var, a_smooth))\n",
    "            print(f\"fold_scores: {fold_scores}\")\n",
    "            count += 1\n",
    "\n",
    "        mean_score = np.mean(fold_scores)\n",
    "        print(mean_score)\n",
    "        if mean_score < best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = (a_var, a_smooth)\n",
    "\n",
    "    print(\"Best parameters:\", best_params, \"with CV loss:\", best_score)\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290cb78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"alpha_var\":   [1.0],\n",
    "    \"alpha_smooth\":[1.0]}\n",
    "\n",
    "import scipy.sparse as sp\n",
    "trial_len = 9\n",
    "best_params, best_score = calculate_weight(param_grid, beta_valume_clean_2d, bold_data, anat_img, mask_2d, trial_len)\n",
    "\n",
    "# L_task, L_var, L_smooth, selected_BOLD_data = calculate_matrices(beta_valume_clean_2d, bold_data, anat_img, mask_2d, None, trial_len)\n",
    "# weights = optimize_voxel_weights(L_task, L_var, L_smooth, alpha_var=best_params[0], alpha_smooth=best_params[1])\n",
    "# weight_img, masked_weights, y = select_opt_weight(selected_BOLD_data, weights, active_low_var_voxels.astype(bool), affine)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_task, L_var, L_smooth, selected_BOLD_data = calculate_matrices(beta_valume_clean_2d, bold_data, anat_img, mask_2d, None, trial_len)\n",
    "weights = optimize_voxel_weights(L_task, L_var, L_smooth, alpha_var=best_params[0], alpha_smooth=best_params[1])\n",
    "y = selected_BOLD_data.T @ weights\n",
    "\n",
    "np.save('weights.npy', weights)\n",
    "np.save('y.npy', y)\n",
    "print(\"Finished!\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
