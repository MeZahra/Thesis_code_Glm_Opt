{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import math\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.sparse import csgraph\n",
    "import cvxpy as cp\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m sub = \u001b[33m'\u001b[39m\u001b[33m04\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m run = \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m mat = \u001b[43mhdf5storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfmri_sub\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msub\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_ses\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mses\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_run\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.mat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# optional: read only what you need\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msqueeze_me\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# optional: drop singleton dims\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m bold_data = mat[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(bold_data), \u001b[38;5;28mgetattr\u001b[39m(bold_data, \u001b[33m'\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis_code_git/.venv/lib/python3.12/site-packages/hdf5storage/__init__.py:2686\u001b[39m, in \u001b[36mloadmat\u001b[39m\u001b[34m(file_name, mdict, appendmat, variable_names, marshaller_collection, options, **keywords)\u001b[39m\n\u001b[32m   2684\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m variable_names:\n\u001b[32m   2685\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m contextlib.suppress(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2686\u001b[39m                 data[k] = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[38;5;66;03m# Read all the variables, stuff them into mdict, and return it.\u001b[39;00m\n\u001b[32m   2688\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mdict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis_code_git/.venv/lib/python3.12/site-packages/hdf5storage/__init__.py:1937\u001b[39m, in \u001b[36mFile.read\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1904\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mFile\u001b[39m\u001b[33m\"\u001b[39m, path: pathesc.Path = \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m   1905\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Read one piece of data from the file.\u001b[39;00m\n\u001b[32m   1906\u001b[39m \n\u001b[32m   1907\u001b[39m \u001b[33;03m    A wrapper around the ``reads`` method to read a single piece of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1935\u001b[39m \n\u001b[32m   1936\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1937\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis_code_git/.venv/lib/python3.12/site-packages/hdf5storage/__init__.py:2008\u001b[39m, in \u001b[36mFile.reads\u001b[39m\u001b[34m(self, paths)\u001b[39m\n\u001b[32m   2006\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCould not find containing Group \u001b[39m\u001b[33m\"\u001b[39m + groupname + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2007\u001b[39m         \u001b[38;5;66;03m# Hand off everything to the low level reader.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2008\u001b[39m         datas.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2009\u001b[39m \u001b[38;5;66;03m# Return it all.\u001b[39;00m\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m datas\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis_code_git/.venv/lib/python3.12/site-packages/hdf5storage/utilities.py:369\u001b[39m, in \u001b[36mLowLevelFile.read_data\u001b[39m\u001b[34m(self, grp, name, dsetgrp)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_modules:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdsetgrp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m m.read_approximate(\u001b[38;5;28mself\u001b[39m, dsetgrp, attributes)\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m hdf5storage.exceptions.CantReadError(\u001b[33m\"\u001b[39m\u001b[33mCould not read \u001b[39m\u001b[33m\"\u001b[39m + dsetgrp.name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis_code_git/.venv/lib/python3.12/site-packages/hdf5storage/marshallers.py:1153\u001b[39m, in \u001b[36mNumpyScalarArrayMarshaller.read\u001b[39m\u001b[34m(self, f, dsetgrp, attributes)\u001b[39m\n\u001b[32m   1146\u001b[39m \u001b[38;5;66;03m# If it is a Dataset, it can simply be read and then acted upon\u001b[39;00m\n\u001b[32m   1147\u001b[39m \u001b[38;5;66;03m# (if it is an HDF5 Reference array, it will need to be read\u001b[39;00m\n\u001b[32m   1148\u001b[39m \u001b[38;5;66;03m# recursively). If it is a Group, then it is a structured\u001b[39;00m\n\u001b[32m   1149\u001b[39m \u001b[38;5;66;03m# ndarray like object that needs to be read field wise and\u001b[39;00m\n\u001b[32m   1150\u001b[39m \u001b[38;5;66;03m# constructed.\u001b[39;00m\n\u001b[32m   1151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dset, h5py.Dataset):\n\u001b[32m   1152\u001b[39m     \u001b[38;5;66;03m# Read the data.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1153\u001b[39m     data = \u001b[43mdset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1155\u001b[39m     \u001b[38;5;66;03m# If it is a reference type, then we need to make an object\u001b[39;00m\n\u001b[32m   1156\u001b[39m     \u001b[38;5;66;03m# array that is its replicate, but with the objects they are\u001b[39;00m\n\u001b[32m   1157\u001b[39m     \u001b[38;5;66;03m# pointing to in their elements instead of just the\u001b[39;00m\n\u001b[32m   1158\u001b[39m     \u001b[38;5;66;03m# references.\u001b[39;00m\n\u001b[32m   1159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m h5py.check_dtype(ref=dset.dtype) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis_code_git/.venv/lib/python3.12/site-packages/h5py/_hl/dataset.py:820\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, args, new_dtype)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fast_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "ses = 1\n",
    "sub = '04'\n",
    "run = 1\n",
    "\n",
    "mat = hdf5storage.loadmat(\n",
    "    f'fmri_sub{sub}_ses{ses}_run{run}.mat',\n",
    "    variable_names=['data'],   # optional: read only what you need\n",
    "    squeeze_me=True            # optional: drop singleton dims\n",
    ")\n",
    "bold_data = mat['data']\n",
    "print(type(bold_data), getattr(bold_data, 'shape', None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses = 1\n",
    "sub = '04'\n",
    "run = 1\n",
    "\n",
    "base_path = '/mnt/TeamShare/Data_Masterfile/H20-00572_All-Dressed/PRECISIONSTIM_PD_Data_Results/fMRI_preprocessed_data/Rev_pipeline/derivatives'\n",
    "anat_img = nib.load(f\"/mnt/TeamShare/Data_Masterfile/H20-00572_All-Dressed/PRECISIONSTIM_PD_Data_Results/fMRI_preprocessed_data/Rev_pipeline/derivatives/sub-pd0{sub}/ses-{ses}/anat/sub-pd0{sub}_ses-{ses}_T1w_brain.nii.gz\")\n",
    "data_name = f'fmri_sub{sub}_ses{ses}_run{run}.mat'\n",
    "# BOLD_path_org = join(base_path, data_name)\n",
    "BOLD_path_org = join(data_name)\n",
    "\n",
    "with h5py.File(BOLD_path_org, 'r') as mat_file:\n",
    "    bold_data = mat_file['data'][()]\n",
    "\n",
    "# Matlab v7.3 stores arrays with time as the leading axis; align with beta volumes\n",
    "bold_data = np.asarray(bold_data)\n",
    "bold_data = np.transpose(bold_data, (3, 2, 1, 0))\n",
    "\n",
    "# mask_path = f'/mnt/TeamShare/Data_Masterfile/H20-00572_All-Dressed/PRECISIONSTIM_PD_Data_Results/fMRI_preprocessed_data/Rev_pipeline/derivatives/sub-pd0{sub}/ses-{ses}/anat/sub-pd0{sub}_ses-{ses}_T1w_brain_mask.nii.gz'\n",
    "# back_mask = nib.load(mask_path)\n",
    "# mask_path = f'/mnt/TeamShare/Data_Masterfile/H20-00572_All-Dressed/PRECISIONSTIM_PD_Data_Results/fMRI_preprocessed_data/Rev_pipeline/derivatives/sub-pd0{sub}/ses-{ses}/anat/sub-pd0{sub}_ses-{ses}_T1w_brain_pve_0.nii.gz'\n",
    "# csf_mask = nib.load(mask_path)\n",
    "# print(anat_img.shape, bold_data.shape, back_mask.shape, csf_mask.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_mask_data = back_mask.get_fdata() > 0\n",
    "csf_mask_data = csf_mask.get_fdata() > 0\n",
    "mask = np.logical_and(back_mask_data, ~csf_mask_data)\n",
    "nonzero_mask = np.where(mask)\n",
    "masked_bold = bold_data[nonzero_mask]\n",
    "print(f\"number of selected voxels after masking: {masked_bold.shape[0]/math.prod(bold_data.shape[:3])*100:.2f}%\")\n",
    "print('bold_data masked shape:', masked_bold.shape)\n",
    "\n",
    "glm_dict = np.load(f'/scratch/st-mmckeown-1/zkavian/fmri_models/TYPED_FITHRF_GLMDENOISE_RR.npy', allow_pickle=True).item()\n",
    "beta_glm = glm_dict['betasmd']\n",
    "beta_run1, beta_run2 = beta_glm[:,0,0,:90], beta_glm[:,0,0,90:]\n",
    "R2_run1, R2_run2 = glm_dict['R2run'][:,:,:,0], glm_dict['R2run'][:,:,:,1]\n",
    "beta = beta_run1 #beta_glm.shape\n",
    "R2 = R2_run1\n",
    "lower_thr, upper_thr = np.nanpercentile(beta, [1, 99])\n",
    "print(f'low_thr: {lower_thr:.2f}, high_thr: {upper_thr:.2f}') #low_thr: -4.64, high_thr: 4.60\n",
    "beta_extreme_mask = np.logical_or(beta < lower_thr, beta > upper_thr)\n",
    "voxels_with_extreme_beta = np.any(beta_extreme_mask, axis=1)\n",
    "print(f\"percentage of voxels with extreme beta values: {np.sum(voxels_with_extreme_beta)/beta.shape[0]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrices(beta_valume_clean_2d, bold_data, anat_img, mask_2d, trial_indices=None, trial_len=9):\n",
    "    num_trials = beta_valume_clean_2d.shape[-1]\n",
    "    if trial_indices is None:\n",
    "        trial_idx = np.arange(num_trials)\n",
    "    else:\n",
    "        trial_idx = np.asarray(trial_indices, dtype=int).ravel()\n",
    "        trial_idx = np.unique(trial_idx)\n",
    "\n",
    "    beta_selected = beta_valume_clean_2d[:, trial_idx]\n",
    "\n",
    "    mean_beta_filtered = np.nanmean(beta_selected, axis=-1)\n",
    "    L_task = np.zeros_like(mean_beta_filtered)\n",
    "    np.divide(1.0, np.abs(mean_beta_filtered), out=L_task, where=mean_beta_filtered != 0)\n",
    "\n",
    "###\n",
    "    bold_data_reshape = np.reshape(bold_data, (-1, bold_data.shape[-1]))\n",
    "    bold_data_selected = bold_data_reshape[~mask_2d]\n",
    "\n",
    "    bold_data_selected_reshape = np.zeros((bold_data_selected.shape[0], num_trials, trial_len), dtype=bold_data_selected.dtype)\n",
    "    start = 0\n",
    "    for i in range(num_trials):\n",
    "        end = start + trial_len\n",
    "        if end > bold_data_selected.shape[1]:\n",
    "            raise ValueError(\"BOLD data does not contain enough timepoints for all trials\")\n",
    "        bold_data_selected_reshape[:, i, :] = bold_data_selected[:, start:end]\n",
    "        start += trial_len\n",
    "        if start == 270 or start == 560:\n",
    "            start += 20\n",
    "\n",
    "    selected_BOLD_data_subset = bold_data_selected_reshape[:, trial_idx, :]\n",
    "\n",
    "    diff_mat = np.diff(selected_BOLD_data_subset, axis=1)\n",
    "    diff_mat_flat = diff_mat.reshape(diff_mat.shape[0], -1)\n",
    "    L_var = np.cov(diff_mat_flat, bias=False, dtype=np.float32)\n",
    "    L_var = (L_var + L_var.T) / 2 + 1e-6 * np.eye(L_var.shape[0], dtype=np.float32)\n",
    "####\n",
    "\n",
    "    mask_selected = (~mask_2d).reshape(bold_data.shape[:3])\n",
    "    selected_linear_idx = np.flatnonzero(mask_selected)\n",
    "\n",
    "    voxel_indices_local = np.column_stack(np.unravel_index(selected_linear_idx, bold_data.shape[:3]))\n",
    "    voxel_indices = voxel_indices_local\n",
    "    selected_world_coords = nib.affines.apply_affine(anat_img.affine, voxel_indices)\n",
    "    D = cdist(selected_world_coords, selected_world_coords)\n",
    "    nonzero = D[D > 0]\n",
    "    sigma = np.median(nonzero) if nonzero.size else 1.0\n",
    "    W = np.exp(-D**2 / (2.0 * sigma**2))\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    L_smooth = csgraph.laplacian(W, normed=False)\n",
    "\n",
    "    selected_BOLD_flat = selected_BOLD_data_subset.reshape(selected_BOLD_data_subset.shape[0], -1)\n",
    "\n",
    "    return L_task, L_var, L_smooth, selected_BOLD_flat\n",
    "\n",
    "def objective_func(w, L_task, L_var, L_smooth, alpha_var, alpha_smooth):\n",
    "    quad = (w.T @ np.diag(L_task) @ w + alpha_var * (w.T @ L_var @ w) + alpha_smooth * (w.T @ L_smooth @ w))\n",
    "    return quad\n",
    "\n",
    "def optimize_voxel_weights(L_task, L_var, L_smooth, alpha_var, alpha_smooth):\n",
    "    L_total = np.diag(L_task) + alpha_var * L_var + alpha_smooth * L_smooth\n",
    "    n = L_total.shape[0]\n",
    "    L_total = np.nan_to_num(L_total)\n",
    "    L_total = 0.5*(L_total + L_total.T) + 1e-8*np.eye(n)\n",
    "    w = cp.Variable(n, nonneg=True)\n",
    "    constraints = [cp.sum(w) == 1]\n",
    "\n",
    "    # objective = cp.Minimize(cp.quad_form(w, L_total) + alpha_sparse * cp.norm1(w))\n",
    "    objective = cp.Minimize(cp.quad_form(w, L_total))\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver=cp.OSQP, verbose=True)\n",
    "    return w.value\n",
    "\n",
    "\n",
    "def calculate_weight(param_grid, beta_valume_clean_2d, bold_data, anat_img, mask_2d, trial_len):\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=0)\n",
    "    best_score = np.inf\n",
    "    best_params = None\n",
    "    num_trials = beta_valume_clean_2d.shape[-1]\n",
    "\n",
    "    for a_var, a_smooth in product(*param_grid.values()):\n",
    "        fold_scores = []\n",
    "        print(f\"a_var: {a_var}, a_smooth: {a_smooth}\")\n",
    "        count = 1\n",
    "\n",
    "        for train_idx, val_idx in kf.split(np.arange(num_trials)):\n",
    "            print(f\"k-fold num: {count}\")\n",
    "            L_task_train, L_var_train, L_smooth_train, _ = calculate_matrices(beta_valume_clean_2d, bold_data, anat_img, mask_2d, train_idx, trial_len)\n",
    "            w = optimize_voxel_weights(L_task_train, L_var_train, L_smooth_train, alpha_var=a_var, alpha_smooth=a_smooth)\n",
    "\n",
    "            L_task_val, L_var_val, L_smooth_val, _ = calculate_matrices(beta_valume_clean_2d, bold_data, anat_img, mask_2d, val_idx, trial_len)\n",
    "\n",
    "            fold_scores.append(objective_func(w, L_task_val, L_var_val, L_smooth_val, a_var, a_smooth))\n",
    "            print(f\"fold_scores: {fold_scores}\")\n",
    "            count += 1\n",
    "\n",
    "        mean_score = np.mean(fold_scores)\n",
    "        print(mean_score)\n",
    "        if mean_score < best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = (a_var, a_smooth)\n",
    "\n",
    "    print(\"Best parameters:\", best_params, \"with CV loss:\", best_score)\n",
    "    return best_params, best_score\n",
    "\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_volume_filter = np.load(\"beta_volume_filter.npy\")\n",
    "spatial_shape = beta_volume_filter.shape[:-1]\n",
    "voxels_with_any_nan = np.zeros(spatial_shape, dtype=bool)\n",
    "voxels_with_all_nan = np.ones(spatial_shape, dtype=bool)\n",
    "\n",
    "# Sweep the time dimension once\n",
    "for t in range(beta_volume_filter.shape[-1]):\n",
    "    frame_nan = np.isnan(beta_volume_filter[..., t])\n",
    "    voxels_with_any_nan |= frame_nan\n",
    "    voxels_with_all_nan &= frame_nan\n",
    "\n",
    "print(np.sum(voxels_with_any_nan), np.sum(voxels_with_all_nan))\n",
    "\n",
    "n_trial = beta_volume_filter.shape[-1]\n",
    "beta_volume_filter_2d = beta_volume_filter.reshape(-1, n_trial)\n",
    "print(beta_volume_filter_2d.shape)\n",
    "mask_2d = voxels_with_all_nan.reshape(-1)\n",
    "beta_valume_clean_2d = beta_volume_filter_2d[~mask_2d]\n",
    "print(beta_valume_clean_2d.shape)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"alpha_var\":   [0.01, 0.5, 1.0],\n",
    "    \"alpha_smooth\":[0.01, 0.5, 1.0]}\n",
    "\n",
    "trial_len = 9\n",
    "best_params, best_score = calculate_weight(param_grid, beta_valume_clean_2d, bold_data, anat_img, mask_2d, trial_len)\n",
    "L_task, L_var, L_smooth, selected_BOLD_data = calculate_matrices(beta_valume_clean_2d, bold_data, anat_img, mask_2d, None, trial_len)\n",
    "weights = optimize_voxel_weights(L_task, L_var, L_smooth, alpha_var=best_params[0], alpha_smooth=best_params[1])\n",
    "y = selected_BOLD_data.T @ weights\n",
    "\n",
    "np.save('weights.npy', weights)\n",
    "np.save('y.npy', y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
